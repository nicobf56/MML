\subsubsection{GRU}

\noindent
Gated recurrent Units (GRUs) are part of the recurrent neural network (RNNs) designed to work with sequential data. GRUs use a gating mechanisms with update and reset gates to control flow information over time, which allows the model to remember or forget information effectively. This helps to capture relationships in text while having a less complex structure like for example LSTM networks.

GRUs have shown strong performance when in use for hate speech detection in social media. In a Comparison between Machine Learning and Deep Learning Approaches for the Detection of Toxic Comments on Social Networks \citep{bonetti2023comparison}, GRUs are compared with LSTM and CNN models, in this case GRUs performed well in accuracy and adaptability, especially with diverse and informal language. Another study Hate Speech Detection in Social Networks using Machine Learning and Deep Learning Methods highlighted GRUs efficiency in recognizing patterns in toxic comments, achieving a high F1-score of 0.907 compared to traditional methods. GRUs are efficient for real-time content moderation, balancing performance and speed.