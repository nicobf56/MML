\subsubsection{MLP}
An MLP (Multi-Layer Perceptron) is the simplest and most fundamental form of neural networks and deep learning. It builds on the concept of neurons, grouping them into layers and then stacking those layers. Each neuron performs a simple weighted sum, but when organized in layers, the input data can be transformed into a new feature space. MLP architectures typically consist of an input layer, multiple hidden layers, and an output layer, which is responsible for producing the final result. These networks are usually fully connected, meaning that every output from one layer is passed to each node in the next layer. Layers are paired with activation functions that introduce non-linearity and help transform outputs into a desired range, such as between 0 and 1 to represent a probability. MLPs are trained using an ``error'' metric, such as cross-entropy loss, and optimized using algorithms like gradient descent through backpropagation across all weights~\cite{mlp_gfg}.
