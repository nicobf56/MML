\subsubsection{GRU Methodology}
\label{sec:gru}
For the GRU model, we began with the use of pre-trained embeddings where each sentence and sequence of words were represented as a fixed length word. Each input sample was structured as matrix of shapes, capturing the semantic composition of text. The sequences where then fit into the GRU layer where it captured temporal dependences and contextual information. Then the output was passed through fully connected layers with drop out regularization of 0.3 to maintain learning rate as the model should not be as complex. The second GRU layer returns the final hidden state, condensing sequential context into a vector, just right before another Dropout of 0.3.  Finally the output is goes through a fully connected dense layer with 64 units and ReLU activation to introduce non-linearity and learn higher-level features. A final dense layer with sigmoid activation outputs a binary probability and the model is trained using the Adam optimizer with binary cross-entropy loss and accuracy as the evaluation metric.